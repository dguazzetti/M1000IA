# -*- coding: utf-8 -*-
"""TP_IA_Tecnologas_Grupo_10_Modelo_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-p88FuWjFgqsSILjNtxml-SkCAS2Kok

# Trabajo - Prediccion de desperdicios alimenticios

---

## Integrantes

- Melisa Garcia Martino

- Yerika Marley Basto Rozo

- Daiana Guazzetti

- Leidy Mora Vaca

- Laura Moyano

- Pamela Heredia

- Debora Acuña

# Introduccion

Se eligio para trabajar la tematica de desperdicio de alimentos a nivel mundial. La informacion trabajada fue obtenida de kaggle. La misma cuenta con dos datasets:

- Data.csv : contiene mas de 25416 filas, 18 columnas
- FoodWasteDataAndResearchByCountry.csv : 214 filas, 12 columnas.

Data.csv, columnas :

- m49_code: codigo de region/pais                
- country                   
- region                     
- cpc_code: codigo de aduana                 
- commodity: Maiz, Mijo, Sorgo, Arroz, tomate, entre otros.                  
- year: del 2000 al 2022.                        
- loss_percentage: variable target expresada c/100.            
- loss_percentage_original: variable target expresada en %   
- loss_quantity: cantidad en kgs de alimentos perdida.             
- activity: Storage(en Deposito), Transport(Transporte), Drying(Secado), Harvest(Cosecha), entre otros.                 
- food_supply_stage : Farm(Siembra); Harvest(Cosecha);Storage(Deposito);Transport (Transporte);Retail (venta minosita);Wholesale(venta al por mayor).          

- treatment                 
- cause_of_loss: Perdida Fisica, Peste de insectos, daño mecanico, entre otros.              
- sample_size                
- method_data_collection: metodo utilizado para la colecta de informacion, modelado de informacion, encuestas, noticias, organismos publicos, experimentos controlados.    
- reference: referencia dela fuente de la informacion.                  
- url: contiene url de informacion relacionada con el desperdidio de alimentos                        
- notes: contiene aclaraciones de cada caso


Food.csv columnas:

- Country                                    
- combined figures (kg/capita/year): suma de household + retail en kgs + servicios de comida.          
- Household estimate (kg/capita/year): consumo por familia          
- Household estimate (tonnes/year)             
- Retail estimate (kg/capita/year) : consumo en mercado minorista             
- Retail estimate (tonnes/year)               
- Food service estimate (kg/capita/year): consumo de lugares de servicio de comida      
- Food service estimate (tonnes/year)          
- Confidence in estimate: confianza del estudio                  
- M49 code: codigo de pais/region                                 
- Region                                  
- Source: fuente de informacion, contiene una unica url

# Imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.regression import linear_model
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from statsmodels.tools import eval_measures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error , r2_score
from sklearn.metrics import accuracy_score,recall_score,precision_score, f1_score, auc, roc_curve, confusion_matrix, classification_report
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import statsmodels.tsa.api as smt
import statsmodels.api as sm
from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier
from sklearn import tree
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

"""# Funciones útiles"""

# Funcion para normalizar nombres de las columnas, aplica para los nuevos data frames encontratos para hacer merge

def normalize_columns(dataset, ds_name=None):
  # Reemplazar todas las mayúsculas en los nombres de las columnas por minúsculas y los espacios por guiones bajos en production_df
  dataset.columns = dataset.columns.str.lower()
  dataset.columns = dataset.columns.str.replace(' ', '_')

  if not ds_name == None:
    #renombrar columnas para poder realizar el merge
    dataset = dataset.rename(columns={'area_code_(m49)': 'm49_code'})
    dataset = dataset.rename(columns={'year_code': 'year'})
    dataset = dataset.rename(columns={'element_code': ds_name+'_element_code'})
    dataset = dataset.rename(columns={'item_code': ds_name+'_item_code'})
    dataset = dataset.rename(columns={'value': ds_name+'_value'})
    dataset = dataset.rename(columns={'element': ds_name+'_element'})
    dataset = dataset.rename(columns={'item': ds_name+'_item'})
    dataset = dataset.rename(columns={'domain': ds_name+'_domain'})
    dataset = dataset.rename(columns={'domain_code': ds_name+'_domain_code'})

  return dataset

"""# Load data"""

# file url

file = "https://raw.githubusercontent.com/melisagarciamartino/Tecnologas-IA/main/Data.csv"

file2 = "https://raw.githubusercontent.com/melisagarciamartino/Tecnologas-IA/main/FoodWasteDataAndResearchByCountry.csv"

"""# Análisis Exploratorio"""

df_base_data_original = pd.read_csv(file, delimiter=',')

df_base_food_original = pd.read_csv(file2, delimiter=',')

df_base_data = df_base_data_original.copy()
df_base_food = df_base_food_original.copy()

df_base_data = normalize_columns(df_base_data, 'base_data')

df_base_data.head(5)

df_base_data["cpc_code"].value_counts()  #la transformamos en variable numerica

df_base_data["cpc_code"] = df_base_data["cpc_code"].replace("21116i",0) #se reemplaza porque era lo que no permitia transformar col a float

df_base_data["cpc_code"] = df_base_data["cpc_code"].astype('float64')

df_base_data["reference"].head(1) #para ver que hay en reference

df_base_food.head(5) # combined figures es la suma por kg de houshold, retail y food_service, se va a utilizar este df para tener datos de region con el m49 y para tener datos de demanda de alimentos.

df_base_data.info()

df_base_data.isnull().sum()

df_base_data.shape

df_base_food.info()

df_base_food = normalize_columns(df_base_food, 'data_food')

df_base_food.isnull().sum()

df_base_food.shape

"""# Correlaciones

###Data set food
"""

df_base_food[['food_service_estimate_(tonnes/year)','household_estimate_(tonnes/year)','retail_estimate_(tonnes/year)']].corr()

plt.figure(figsize=(10,10))
sns.heatmap(df_base_food[['food_service_estimate_(tonnes/year)','household_estimate_(tonnes/year)','retail_estimate_(tonnes/year)']].corr().round(1), annot=True, vmin=-1, cmap='Reds');

len(df_base_food.m49_code.unique()) #al ya tener 214 m49 igual al largo del data set ya los datos estan agrupados

"""# Merge con Data set Base y Data Set Food

La idea es poder tener un dato de la demanda de alimentos estimada agrupada por M49 uego realizar todo el EDA en busca de relaciones
"""

df_base_data.shape

df_base_data = pd.merge(df_base_data, df_base_food[['m49_code','food_service_estimate_(tonnes/year)','household_estimate_(tonnes/year)','retail_estimate_(tonnes/year)']], on=['m49_code'])
df_base_data.shape

df_base_data.info()

"""## Data set Base unificado con Food"""

sns.heatmap(df_base_data.select_dtypes(np.number).corr().round(2), annot=True, vmin=-1, cmap='Reds')

"""### Pairplot de variables numericas en data set de data global"""

sns.pairplot(df_base_data)

"""### Relacion entre loss_percentage y años"""

sns.lineplot(df_base_data, x="year", y="loss_percentage")

"""## Relacion entre loss_percentage y paises

### Cantidad de datos por pais en frecuencia absoluta (mas adelante en frecuencia relativa)
"""

repeted_countries_num = df_base_data.country.value_counts().values

country_names = df_base_data.country.value_counts().index

first_23_countries = country_names[:23]
countries_23_to_48= country_names[23:48]
countries_48_to_73 = country_names[48:73]
countries_73_to_98= country_names[73:98]
countries_98_to_123 = country_names[98:123]

rep_countries_first_23  = repeted_countries_num[:23]
rep_countries_23_to_48= repeted_countries_num[23:48]
rep_countries_48_to_73 = repeted_countries_num[48:73]
rep_countries_73_to_98= repeted_countries_num[73:98]
rep_countries_98_to_123 = repeted_countries_num[98:123]

fig, (ax1,ax2) = plt.subplots(2,1,figsize=(5, 10))

#fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(5,1,figsize=(10, 30))

sns.barplot(x = rep_countries_first_23, y = first_23_countries,ax=ax1)
sns.barplot(x = rep_countries_23_to_48, y = countries_23_to_48,ax=ax2)
#sns.barplot(x = rep_countries_48_to_73, y = countries_48_to_73,ax=ax3)
#sns.barplot(x = rep_countries_73_to_98, y = countries_73_to_98,ax=ax4)
#sns.barplot(x = rep_countries_98_to_123, y = countries_98_to_123,ax=ax5)

plt.show()

"""### Suma de loss_percentage por país"""

pivot_loss_percentage = df_base_data.groupby('country')['loss_percentage'].sum().sort_values(ascending=False).round(2)

pivot_loss_percentage

loss_percentage_amount_country = pivot_loss_percentage.index #son los paises de la pivot table de arriba

#son las filas con la suma del loss_percentage por pais
loss_percentage_first_23  = pivot_loss_percentage[:23]
loss_percentage_23_to_48= pivot_loss_percentage[23:48]
loss_percentage_48_to_73 = pivot_loss_percentage[48:73]
loss_percentage_73_to_98= pivot_loss_percentage[73:98]
loss_percentage_98_to_123 = pivot_loss_percentage[98:123]

loss_p_first_23_country_name = loss_percentage_amount_country[:23]
loss_p_23_to_48_country_name= loss_percentage_amount_country[23:48]
loss_p_48_to_73_country_name = loss_percentage_amount_country[48:73]
loss_p_73_to_98_country_name= loss_percentage_amount_country[73:98]
loss_p_98_to_123_country_name = loss_percentage_amount_country[98:123]

#grafico con lo constriudo de suma de loss_percentage por pais
fig, (ax1,ax2) = plt.subplots(2,1,figsize=(5, 10))

#fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(5,1,figsize=(10, 30))

sns.barplot(x = loss_percentage_first_23, y = loss_p_first_23_country_name,ax=ax1)
sns.barplot(x = loss_percentage_23_to_48, y = loss_p_23_to_48_country_name,ax=ax2)
#sns.barplot(x = loss_percentage_48_to_73, y = loss_p_48_to_73_country_name,ax=ax3)
#sns.barplot(x = loss_percentage_73_to_98, y = loss_p_73_to_98_country_name,ax=ax4)
#sns.barplot(x = loss_percentage_98_to_123, y = loss_p_98_to_123_country_name,ax=ax5)

plt.show()

"""## Categorizacion de loss_percentage en grupos: "bajo";"medio";"alto";"muy alto"

Esto se realiza a fin de poder encarar el problema tanto como un problema de regresion y como un problema de clasificacion
"""

group_labels = ["bajo", "medio", "alto", "muy alto"]

df_base_data["loss_percentage_categories"] = pd.qcut(df_base_data.loss_percentage,q = 4, labels = group_labels)

df_base_data["loss_percentage_categories"]

"""### Frecuencia relativa de loss_percentage categorizada
 Pareciera no haber problema de desbalanceo de clases
"""

df_base_data["loss_percentage_categories"].value_counts(normalize = True)

"""## Limpieza de columnas en data set data"""

df_base_data.isnull().sum()

df_base_data_not_nulls = df_base_data[["m49_code" ,"country","cpc_code", "commodity" , "year" , "loss_percentage" , "activity" ,"food_supply_stage","method_data_collection","loss_percentage_categories","food_service_estimate_(tonnes/year)","household_estimate_(tonnes/year)","retail_estimate_(tonnes/year)"]]

#dropped "region" ,"loss_quantity" , "treatment" , "cause_off_loss" , "sample_size" , "reference" , "notes"

df_base_data_not_nulls.isnull().sum()

df_base_data_not_nulls.sample(10)

df_base_data_not_nulls["activity"]= df_base_data_not_nulls["activity"].fillna(0)
df_base_data_not_nulls["food_supply_stage"]= df_base_data_not_nulls["food_supply_stage"].fillna(0)
df_base_data_not_nulls["method_data_collection"]= df_base_data_not_nulls["method_data_collection"].fillna(0)#saco las valores con NA

df_base_data_not_nulls.isnull().sum()

df_base_data_not_nulls.info()

"""## Division en variables categoricas y numericas dentro del dataset"""

categorical = df_base_data_not_nulls.select_dtypes(include= ["object","category"])
numeric = df_base_data_not_nulls.select_dtypes(np.number)

"""## Analisis general de variables cualitativas"""

plt.figure(figsize=(20,55))

for i,var in enumerate(categorical):
    plt.subplot(7,1,i+1)
    df_base_data_not_nulls[var].value_counts(normalize=True).plot(kind='bar',color="skyblue", title=var)

"""# Filtrado de informacion


"""

commodity_mask = df_base_data_not_nulls['commodity'].value_counts(normalize = True) >3/100
commodity_mask[:20]

commodity_mask = (df_base_data_not_nulls['commodity'] == 'Maize (corn)') | (df_base_data_not_nulls['commodity'] == 'Rice') | (df_base_data_not_nulls['commodity'] == 'Millet')| (df_base_data_not_nulls['commodity'] == 'Sorghum')| (df_base_data_not_nulls['commodity'] == 'Wheat')#| (df_base_data_not_nulls['commodity'] == 'Fonio') | (df_base_data_not_nulls['commodity'] == 'Barley') | (df_base_data_not_nulls['commodity'] == 'Tomatoes') | (df_base_data_not_nulls['commodity'] == 'Potatoes') | (df_base_data_not_nulls['commodity'] == 'Mangoes, guavas and mangosteens')

df_base_data_not_nulls = df_base_data_not_nulls[commodity_mask]

#con los commodities que trabajaremos
df_base_data_not_nulls['commodity'].value_counts(normalize = True)

df_base_data_not_nulls.shape[0] / df_base_data_original.shape[0]

sns.heatmap(df_base_data_not_nulls.select_dtypes(np.number).corr().round(2), annot=True, vmin=-1, cmap='Reds');



df_base_data_not_nulls['activity'].value_counts(normalize = True)[:8]

activity_mask = (df_base_data_not_nulls['activity'] == 'Storage') | (df_base_data_not_nulls['activity'] == 'Transportation') | (df_base_data_not_nulls['activity'] == 'Drying, Harvesting')| (df_base_data_not_nulls['activity'] == 'Winnowing')| (df_base_data_not_nulls['activity'] == 'Drying')

df_base_data_not_nulls = df_base_data_not_nulls[activity_mask]

df_base_data_not_nulls.shape[0] / df_base_data_original.shape[0]

#con las actividades que se va a trabajar
df_base_data_not_nulls['activity'].value_counts(normalize = True)

df_base_data_not_nulls['food_supply_stage'].value_counts(normalize = True)[:8]

food_supply_mask = (df_base_data_not_nulls['food_supply_stage'] == 'Farm') | (df_base_data_not_nulls['food_supply_stage'] == 'Harvest') | (df_base_data_not_nulls['food_supply_stage'] == 'Storage')| (df_base_data_not_nulls['food_supply_stage'] == 'Transport')

df_base_data_not_nulls = df_base_data_not_nulls[food_supply_mask]

df_base_data_not_nulls.shape[0] / df_base_data_original.shape[0]

#con las supply stage que se va a trabajar
df_base_data_not_nulls['food_supply_stage'].value_counts(normalize = True)

df_base_data_not_nulls.shape[0] / df_base_data_original.shape[0]

"""# Análisis de variables cualitativas post filtrado"""

plt.figure(figsize=(20,55))

for i,var in enumerate(categorical):
    plt.subplot(7,1,i+1)
    df_base_data_not_nulls[var].value_counts(normalize=True).plot(kind='bar',color="skyblue", title=var)

"""## Veamos si se puede observar un comportamiento relevante entre las variables cualitativas y la variable de *repuesta*

En esta seccion notamos que hay commodities de los cuales se tiene poca informacion y agrupan un gran % de loss_percentage, esto da a pensar que son outliers
"""

pivot_loss_percentage_stage_commodity = df_base_data_not_nulls.groupby(['food_supply_stage' , 'commodity'])[['loss_percentage']].mean().sort_values(ascending=False, by="loss_percentage").round(2)

mask_greater_than = pivot_loss_percentage_stage_commodity['loss_percentage'] > 3

pivot_loss_percentage_stage_commodity_depured  = pivot_loss_percentage_stage_commodity[mask_greater_than]
pivot_loss_percentage_stage_commodity_depured

pivot_loss_percentage_categories =df_base_data_not_nulls.groupby(['loss_percentage_categories','year'])[['loss_percentage']].mean().sort_values(ascending=False, by="loss_percentage").round(2)

pivot_loss_percentage_categories_mask = pivot_loss_percentage_categories['loss_percentage'] > 7

pivot_loss_percentage_categories_depured = pivot_loss_percentage_categories[pivot_loss_percentage_categories_mask]
pivot_loss_percentage_categories_depured

pivot_loss_percentage_activity = df_base_data_not_nulls.groupby(['activity'])[['loss_percentage']].mean().sort_values(ascending=False, by="loss_percentage").round(2)

pivot_loss_percentage_activity_mask = pivot_loss_percentage_activity['loss_percentage'] > 3

pivot_loss_percentage_activity_depured = pivot_loss_percentage_activity[pivot_loss_percentage_activity_mask]
pivot_loss_percentage_activity_depured

"""## Analisis de variables cuantitativas

En esa seccion solamente tenemos la variable m49_code que se corresponde con el Area y la variable target loss_percentage
"""

df_base_data_not_nulls[numeric.columns].info()

df_base_data_not_nulls[numeric.columns].describe()

"""### Box-Plot de la variable Target

A priori se visualiza que a partir de 7% de perdida son outliers --> se elige igualmente filtrar por subarea en lugar de filtrar todo por la target
"""

plt.figure(figsize=(10,5))

sns.boxplot(y= "loss_percentage",data= df_base_data_not_nulls)

"""## Agrupacion por Continente"""

#Diccionario para hacer el map de region del continente
diccionario_region = dict(zip(list(df_base_food['m49_code']), list(df_base_food['region'])))

#Diccionario de Continentes

diccionario_continente = {
"Sub-Saharan Africa" : "Africa",
"Northern Africa" : "Africa",
"Western Africa" : "Africa",
"Africa" : "Africa",
"Southern Asia" : "Asia",
"Eastern Asia" : "Asia",
"Western Asia" : "Asia",
"South-Eastern Asia": "Asia",
"South-eastern Asia" : "Asia",
"Central Asia" : "Asia",
"Northern America" : "America",
"Latin America and the Caribbean" : "America",
"Eastern Europe": "Europe",
"Northern Europe" : "Europe",
"Western Europe" : "Europe",
"Europe" :"Europe",
"Southern Europe": "Europe",
"Australia and New Zealand" : "Oceania",
"Melanesia" : "Oceania"
}

df_base_data_not_nulls['area_continente'] = df_base_data_not_nulls.apply(lambda x: diccionario_region[x['m49_code']] if x['m49_code'] in diccionario_region else x['country'] , axis = 1)

df_base_data_not_nulls['area_continente'].value_counts()

df_base_data_not_nulls['continente'] = df_base_data_not_nulls['area_continente'].apply(lambda x: diccionario_continente[x])

df_base_data_not_nulls['continente'].value_counts()

sns.countplot(x="continente", data=df_base_data_not_nulls , order = df_base_data_not_nulls['continente'].value_counts().index,stat = "proportion")

fig, ax = plt.subplots(1,1,figsize=(45, 10))

sns.countplot(x="area_continente", data=df_base_data_not_nulls , order = df_base_data_not_nulls['area_continente'].value_counts().index,stat = "proportion")

# boxplot
plt.figure(figsize=(10,5))
sns.boxplot(x='continente', y='loss_percentage', data= df_base_data_not_nulls)
plt.show()

plt.figure(figsize=(40,15))
sns.boxplot(x='area_continente', y='loss_percentage', data= df_base_data_not_nulls)
plt.show()

"""# Filtrado de Outliers por subarea de continente

Se vuelve a graficar para que se visualice que como esta distribuida la informacion no sufre de modificacion significativa
"""

#Filtros de outliers segun boxplot
easter_europe_outliers = (df_base_data_not_nulls['area_continente'] == 'Eastern Europe') & (df_base_data_not_nulls['loss_percentage'] < 30)
australia_outliers = (df_base_data_not_nulls['area_continente'] == 'Australia and New Zealand') & (df_base_data_not_nulls['loss_percentage'] < 60)
south_east_asia_outliers = (df_base_data_not_nulls['area_continente'] == 'South-eastern Asia') & (df_base_data_not_nulls['loss_percentage'] < 15)
northern_europe_outliers = (df_base_data_not_nulls['area_continente'] == 'Northern Europe') & (df_base_data_not_nulls['loss_percentage'] < 40)
eastern_asia_outliers = (df_base_data_not_nulls['area_continente'] == 'Eastern Asia') & (df_base_data_not_nulls['loss_percentage'] < 3)
latin_america_outliers = (df_base_data_not_nulls['area_continente'] == 'Latin America and the Caribbean') & (df_base_data_not_nulls['loss_percentage'] < 45)
europe_outliers = (df_base_data_not_nulls['area_continente'] == 'Europe') & (df_base_data_not_nulls['loss_percentage'] < 10)
western_asia_outliers = (df_base_data_not_nulls['area_continente'] == 'Western Asia') & (df_base_data_not_nulls['loss_percentage'] < 35)
southern_asia_outliers = (df_base_data_not_nulls['area_continente'] == 'Southern Asia') & (df_base_data_not_nulls['loss_percentage'] < 7)
northern_america_outliers = (df_base_data_not_nulls['area_continente'] == 'Northern America') & (df_base_data_not_nulls['loss_percentage'] < 18)
northern_africa_outliers = (df_base_data_not_nulls['area_continente'] == 'Northern Africa') & (df_base_data_not_nulls['loss_percentage'] < 7)
subsaharan_africa_outliers = (df_base_data_not_nulls['area_continente'] == 'Sub-Saharan Africa') & (df_base_data_not_nulls['loss_percentage'] < 7)
southeastern_asia_outliers = (df_base_data_not_nulls['area_continente'] == 'South-Eastern Asia') & (df_base_data_not_nulls['loss_percentage'] < 25)

#filtro final

mask_outliers = easter_europe_outliers|australia_outliers|south_east_asia_outliers|northern_europe_outliers|eastern_asia_outliers|latin_america_outliers|europe_outliers|western_asia_outliers|southern_asia_outliers|northern_america_outliers|northern_africa_outliers|subsaharan_africa_outliers|southeastern_asia_outliers

df_base_data_not_nulls = df_base_data_not_nulls[mask_outliers]

plt.figure(figsize=(40,15))
sns.boxplot(x='area_continente', y='loss_percentage', data= df_base_data_not_nulls)
plt.show()

fig, ax = plt.subplots(1,1,figsize=(45, 10))
sns.countplot(x="area_continente", data=df_base_data_not_nulls , order = df_base_data_not_nulls['area_continente'].value_counts().index , stat = "proportion")

df_base_data_not_nulls.shape[0]/df_base_data_original.shape[0]

# boxplot
plt.figure(figsize=(10,5))
sns.boxplot(x='continente', y='loss_percentage', data= df_base_data_not_nulls)
plt.show()

sns.countplot(x="continente", data=df_base_data_not_nulls , order = df_base_data_not_nulls['continente'].value_counts().index,stat = "proportion")

"""#Modelo Final

## Funciones
"""

#FUNCION TRANSFORMACION DUMMIES --> Va a ser el df ya estandarizado y sino hace falta el df base a trabajar
def variables_a_dummies(df):
  df_final_dummies =  pd.get_dummies(df, prefix="dummies", drop_first = True, dtype=int)
  return df_final_dummies

#FUNCION PARA RESETEAR INDICE Y CONCATENAR DF ORIGINAL CON LA X ESTANDARIZADA/CON LA X HECHA DUMMY ANTES
def index_concat(variable_type,df_concat_std,df_concat_dummies):
  df_final = df_concat_std.select_dtypes(np.number).reset_index()
  df_final = pd.concat([df_concat_std,df_concat_dummies],axis=1)
  return df_final

#FUNCION DE ENTRENAMIENTO RANDOM FOREST
def modelo_random(Xtrain, Ytrain):

   random_forest = RandomForestClassifier(n_estimators=100,
                                      criterion='entropy',
                                      #max_depth = 4,
                                      bootstrap=True,
                                      n_jobs = -1,
                                      random_state = 127)
                                      #max_samples= 0.3)

   random_forest = random_forest.fit(Xtrain, Ytrain)
   return random_forest

#FUNCION DE PREDICCION CON MODELO YA ENTRENADO
def predict_random(data,model):

    prediction = model.predict(data)
    return prediction

#FUNCION PARA FEATURE IMPORTANCE
def feature_importance(model,Xtrain):
     importances = model.feature_importances_
     indices = np.argsort(importances)[::-1]
     features = Xtrain.columns

     df_feature_importance = pd.DataFrame()
     df_feature_importance['Features'] = [features[i] for i in indices]
     df_feature_importance['Coeficiente'] = importances[indices]
     df_feature_importance = df_feature_importance.sort_values(by ='Coeficiente',ascending = False)

     return df_feature_importance

#FUNCION METRICAS PERFORMANCE

def performance_categories(Ytest,prediction):
  # accuracy = accuracy_score(Ytest,prediction).round(2)
  accuracy = round(accuracy_score(Ytest,prediction), 2)
  cm = confusion_matrix(Ytest,prediction)

  TP_0 = cm[0,0]
  FP_0_1 = cm[0,1]
  FP_0_2 = cm[0,2]
  FP_0_3 = cm[0,3]

  TP_1 = cm[1,1]
  FN_1_0 = cm[1,0]
  FP_1_2 = cm[1,2]
  FP_1_3 = cm[1,3]

  TP_2 = cm[2,2]
  FN_2_0 = cm[2,0]
  FN_2_1 = cm[2,1]
  FP_2_3 = cm[2,3]

  TP_3 = cm[3,3]
  FN_3_0 = cm[3,0]
  FN_3_1 = cm[3,1]
  FN_3_2 = cm[3,2]

  TP = TP_0 + TP_1 + TP_2 + TP_3
  TN = FP_0_1 + FP_0_2 + FP_0_3 + FP_1_2 + FP_1_3 + FP_2_3
  FP = TN
  FN = FN_1_0 + FN_2_0 + FN_2_1 + FN_3_0 + FN_3_1 + FN_3_2


  precision = (TP / (TP + FP)).round(2) #cuando clasifica en una clase, que tan preciso es o sea si clasifica bien
  recall = (TP / (TP + FN)).round(2) #capacidad de detectar positivos s/los positivos reales
  specificity = (TN / (TN + FP)).round(2)
  f1_score = ((2*precision * recall) / (precision + recall)).round(2)

  df_metricas = pd.DataFrame()
  df_metricas["Modelo"] = ["Random Forest"]
  df_metricas["Accuracy"] = [accuracy]
  df_metricas["Precision"] = [precision]
  df_metricas["Sensitivity(Recall)"] = [recall]
  df_metricas["Specificity"] = [specificity]
  df_metricas["F1"] = [f1_score]

  return df_metricas,cm

"""## Random Forest

## Preparacion de Datos Entrenamiento
"""

df_random = df_base_data_not_nulls.drop(["food_service_estimate_(tonnes/year)","country","continente","m49_code","method_data_collection","loss_percentage","household_estimate_(tonnes/year)","retail_estimate_(tonnes/year)"],axis = 1)

df_random["cpc_code"] = df_base_data_not_nulls["cpc_code"].astype(object)

df_random["year"] = df_base_data_not_nulls["year"].astype(object)

df_dummies = variables_a_dummies(df_random.drop(["loss_percentage_categories"],axis = 1))

loss_percentage_cat = df_random[["loss_percentage_categories"]]

df_random = pd.concat([loss_percentage_cat,df_dummies],axis = 1)

"""### Division en X e Y"""

Y = df_random ["loss_percentage_categories"]
X = df_random .drop(["loss_percentage_categories"], axis=1)

Xtrain, Xtest,Ytrain,Ytest = train_test_split(df_dummies ,Y, random_state = 2)

modelo_fit = modelo_random(Xtrain, Ytrain)

prediction_random_forest = predict_random(Xtest,modelo_fit)

prediction_random_forest

"""###Evaluacion de la Performance del Modelo"""

df_metricas, cm = performance_categories(Ytest,prediction_random_forest)
df_metricas

"""#### Matriz de Confusion + Feature Importance"""

sns.heatmap(cm, annot=True, fmt='.0f')
plt.ylabel('Etiquetas reales')
plt.xlabel('Etiquetas predichas');
plt.title('Confusion Matrix Random')

feature_importance(modelo_fit,Xtrain)

importances = modelo_fit.feature_importances_
indices = np.argsort(importances)[::-1]
features = Xtrain.columns

plt.figure(figsize=(12, 8))
plt.title('Importancia de características - Random Forest')
plt.bar(range(len(importances)), importances[indices], align='center')
plt.xticks(range(len(importances)), [features[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

"""## Conclusion Final

En esta notebook se dejo el modelo final a ser disponibilizado, luego de haber hecho la fase de entrenamiento en una notebook separada habiendo probado predecir el porcentaje de perdida de alimento con una Regresion Multiple optimizada(Lasso) y sin optimizar; y habiendo probado Arbol de Decision y Random Forest para predecir el porcentaje como una etiqueta de "Bajo", "Medio", "Alto", "Muy Alto" se optmo por disponibilizar Random Forest debido a que su performance es mas rapida que la del Arbol de decision y con metricas practicamente identicas a las del arbol.
"""

